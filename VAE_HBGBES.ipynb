{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq-xSyvfoLB6"
   },
   "source": [
    "From: https://keras.io/examples/variational_autoencoder/\n",
    "\n",
    "# Starting from MNIST\n",
    " The idea is customize this to be ready for any \"similar\" dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNng5aT3pMIM"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9WVJF8f1dJV"
   },
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJKStfz8pXb0"
   },
   "source": [
    "## Load the MNIST data and setup the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "QK6v1MxIpTS4",
    "outputId": "75b92804-6e1b-4aae-a025-4ac733fb531f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c1d63175c996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# MNIST dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moriginal_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upvW8BRyppYK"
   },
   "source": [
    "## Do the training and make some plots.\n",
    "There are two loss function options. Not sure yet if one is definitely better than the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3182
    },
    "colab_type": "code",
    "id": "Nm4-xK1hpoYy",
    "outputId": "12eadf33-5f10-40aa-b057-68be4e388a6e"
   },
   "outputs": [],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "\n",
    "# VAE loss = mse_loss or xent_loss + kl_loss\n",
    "reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss = binary_crossentropy(inputs,\n",
    "                                              outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "plot_model(vae,\n",
    "           to_file='vae_mlp.png',\n",
    "           show_shapes=True)\n",
    "\n",
    "\n",
    "# load trained model?\n",
    "if False:\n",
    "    vae.load_weights('vae_mlp_mnist.h5')\n",
    "else:\n",
    "  # train the autoencoder\n",
    "  vae.fit(x_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_test, None))\n",
    "  vae.save_weights('vae_mlp_mnist.h5')\n",
    "\n",
    "plot_results(models,\n",
    "             data,\n",
    "             batch_size=batch_size,\n",
    "             model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fBwCoH-q-wX"
   },
   "source": [
    "You can extract the latent vectors by running: \n",
    "  * **encoder.predict(x_test, batch_size=1)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6OLnw6zgsDa"
   },
   "outputs": [],
   "source": [
    "zb, _, _ = encoder.predict(x_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GQPhiLYWg0-G",
    "outputId": "7f42cd23-cafe-4d8a-f4f7-ff7591427e16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5YOWJ60rfOJ"
   },
   "source": [
    "## If running on Google Colab, you can upload generated files to GDrive like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1u8RXQ-ZhcBg",
    "outputId": "640166b4-1d39-4c32-fedb-2e5a30a7d850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10kB 18.4MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 81kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 92kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 194kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 204kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 215kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 225kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 235kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 245kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 256kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 266kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 276kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 286kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 296kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 307kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 317kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 327kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 337kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 348kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 358kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 368kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 378kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 389kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 399kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 409kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 419kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 430kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 440kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 450kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 460kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 471kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 481kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 491kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 501kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 512kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 522kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 532kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 542kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 552kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 563kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 573kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 583kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 593kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 604kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 614kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 624kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 634kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 645kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 655kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 665kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 675kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 686kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 696kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 706kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 716kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 727kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 737kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 747kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 757kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 768kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 778kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 788kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 798kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 808kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 819kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 829kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 839kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 849kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 860kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 870kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 880kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 890kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 901kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 911kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 921kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 931kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 942kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 952kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 962kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 972kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 983kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 993kB 2.8MB/s \n",
      "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once in a notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once in a notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "uploaded = drive.CreateFile({'title': 'vae_mlp_encoder.png'})\n",
    "uploaded.SetContentFile('vae_mlp_encoder.png')\n",
    "uploaded.Upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLcMzG61j4Fp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE_HBGBES.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
